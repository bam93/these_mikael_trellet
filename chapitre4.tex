\chapter[Lier l'analyse à la visualisation dans un même espace de travail]{Raccourcir la boucle de décision et de conclusion de l'expert}
\label{Sec:visuAna}
\minitoc
\cleardoublepage

% \bibliography{/Users/trellet/Documents/Manuscript_these/these}
%% Commentaire : la commande \texorpdfstring permet de déclarer un titre de
%% chapitre (ou section, sous-section) alternatif en texte seul, si besoin, qui
%% est utilisé par hyperref pour fabriquer un menu dans les fichiers compilés

%\chapter{\texorpdfstring{Contrôle gestuel de l'articulation}{Contrôle gestuel de l'articulation}}
%% Commentaire : la commande \texorpdfstring permet de déclarer un titre de
%% chapitre (ou section, sous-section) alternatif en texte seul, si besoin, qui
%% est utilisé par hyperref pour fabriquer un menu dans les fichiers compilés

%Exemple de notation qui sera reprise dans l'index : soit $\Q$\index{Q@$\Q$} le corps des nombres rationnels.

\section{Introduction}

Dans le schéma standard de l'étude d'un phénomène biologique à l'échelle atomique, la mise en place d'hypothèses de travail se fait suite à l'analyse standardisée d'une simulation moléculaire. Cette simulation, dans le meilleur des cas, aura mis en avant plusieurs pistes de réflexion provenant de changements structuraux ou énergétiques du complexe moléculaire d'intérêt. Cependant, la majorité des simulations amènent que peu d'informations utilisables pour émettre de nouvelles hypothèse et seule une suite de simulations dont les paramètres ont évolué progressivement permet de dégager un résultat significatif. La simulation et son analyse sont deux des quatre tâches identifiées dans la boucle séquentielle de la biologie structurale que nous avons exposé en introduction. La succession de simulation->analyse->nouveaux paramètres->simulation->analyse... est un processus long et coûteux. Tant en terme de quantité de données à traiter qu'en terme de temps de traitement et de communication des données. Notre approche vise à raccourcir ce processus en combinant simulation et analyses afin de:
\begin{enumerate}
    \item Identifier et reconnaître les acteurs biologiques importants d'une simulation de façon rapide mais complète
    \item Réduire la quantité de données générée afin de ne garder que la partie pertinente pour les hypothèses
    \item Apporter à l'utilisateur les outils de décision adaptés en lui fournissant les liens logiques entre ses données
    \item Orienter la simulation vers des paramètres plus pertinents suivant les décisions de l'utilisateur
\end{enumerate}

Notre but est donc de placer l'utilisateur au centre d'une boucle de décision dont les deux principaux fournisseurs de données seraient la simulation en elle-même et ses analyses associées. Ces deux moteurs seraient donc dirigés et directement manipulés par l'utilisateur suivant les hypothèses/conclusions qu'il tirerait des données résultantes de leur combinaison.

% Intro Visu Analytics
\subsection{Visualisation analytique}

L'association étroite entre la visualisation d'informations scientifiques et les analyses associées, dans un même espace et simultanément, met en avant des techniques de visualisation connues et souvent définies en visualisation analytique. Cette discipline récente a en effet pour but de faciliter l'analyse visuelle de données complexes et/ou scientifiques et se définie par le "raisonnement analytique au travers d'interfaces visuelles interactives" \cite{cook_illuminating_2005}. Elle se place à la frontière de nombreux domaines de visualisation, d'interaction ou de perception afin de mettre en avant des informations qui n'auraient pu apparaître lors de l'utilisation cloisonnée des différents domaines mis en jeu. La visualisation analytique s'appuie sur des méthodes de visualisation simples et compréhensibles par l'être humain auxquelles elle ajoute une dimension interactive afin de les relier entre elles. Elle s'inspire donc par plusieurs niveaux aux études d'interactions homme-machine dont la réalité virtuelle s'inspire également \cite{arias-hernandez_visual_2011}. Son principal but est de mettre l'être humain au centre d'une boucle de décision qui sera facilitée par la mise en relation de données de différentes natures et de différentes sources. Ce contexte de travail généré par la visualisation analytique doit être cohérent pour le chercheur. C'est à ce niveau que le domaine de la perception et des études cognitives interviennent afin d'assurer une pleine compréhension et utilisation de l'espace de travail.

La visualisation analytique vise donc à combiner la visualisation de données passées en entrée et leurs analyses afin de créer de nouvelles connaissances qui pourront elles-mêmes être utilisées par la suite en entrée de la boucle. Ce processus, expliqué par Keim et al. \cite{keim2010mastering}, forme ainsi une boucle d'analyse fermée qui vient raccourcir considérablement la boucle habituelle présentée dans la figure X. Une nouvelle représentation de cette boucle intégrant la visualisation analytique peut être observée dans la figure Y.

% Intro sémantique / données liées
\subsection{Représentation des connaissances}

Ce raccourcissement du processus d'étude d'une molécule est aussi motivé par l'espace de travail que l'on lui dédie. En effet, la réalité virtuelle doit par nature se défaire du carcan clavier/souris et mettre en jeu des techniques d'interaction directes plus naturelles. Le nombre d'entrées possibles pour interagir avec l'environnement est réduit comparé aux possibilités en conditions de bureau standards. Chaque commande impliquant l'utilisation d'une combinaison de touche de clavier et/ou souris ne peut être remplacée par un équivalent en terme de boutons de périphérique adapté à l'immersion ou même de commande vocale. Ceci est particulièrement vrai en visualisation moléculaire où les possibilités de rendus et de manipulation des objets sont très nombreuses. En parallèle de la mutualisation des activités visualisation et analyse, il est donc nécessaire de mutualiser également certaines actions afin de réduire le nombre d'étapes d'interaction que l'utilisateur devra effectuer pour arriver à un résultat. Cette mutualisation a fait l'objet de nombreuses recherches en interaction homme-machine et plusieurs solutions matérielles et logicielles existent afin de prendre en compte des mouvements ou des commandes vocales en entrée pour s'affranchir d'un maximum de dispositifs d'interaction qui viendraient réduire l'immersion pour l'utilisateur. 
Lorsqu'on se concentre sur un domaine d'étude en particulier, cette mutualisation est d'autant plus facile que certaines actions sont communes et peuvent donc être définies dans leur totalité à partir d'un minimum d'éléments en entrée. En effet, la connaissance d'un domaine doit permettre de prédire et de fournir des outils dont l'utilisateur aura besoin suivant l'étape du processus dans laquelle il se trouve et ses actions au sein de celle-ci. Comme nous venons de l'énoncer, cela ne peut se faire qu'à travers une excellent connaissance du métier et du domaine. Notre étude nous a donc mené à nous interroger sur la possibilité de formaliser les concepts qui sont utilisés dans le processus d'étude d'une molécule.

La notion de représentation des concepts et des liens entre ces concepts est connue des sciences informatiques et plusieurs formalismes en découlent. La mise en place d'ontologies afin de standardiser les connaissances dans des domaines de recherche scientifique a connu un développement spontané et important à partir de la fin des années 90 \cite{schulze-kremer_ontologies_2002, baker_ontology_1999}. Une ontologie se définit comme un vocabulaire cherchant à classer des concepts et définir des relations entre ces concepts pour un domaine spécifique. Ce classement hiérarchique se doit d'être interprétable à la fois par les machines et les humains. Tout d'abord afin d'être intégrer dans des processus informatiques et ensuite afin de servir de modèle aux scientifiques pour l'élaboration de bases de données résultantes d'ontologies.

La génomique fut le domaine biologique qui a vu le premier la nécessité d'utiliser des ontologies afin d'uniformiser la quantité de données toujours plus importante générée et intégrée dans des bases de données hétérogènes à travers le monde \cite{schuurman_ontologies_2008}.
En génomique et protéomique, de nombreuses études s'appuient sur l'aide directe ou indirecte de \textit{Gene Ontology} \cite{ashburner_gene_2000}, une "bio-ontologie" qui a résulté de la prolifération de jeux de données à l'échelle génomique et la mise en place de protocoles pour l'échange et le partage de données sur le web. Elle fut le fruit de la nécessité, lors de l'explosion du nombre de données générées, de mettre en place un vocabulaire standard que les scientifiques pourraient utiliser afin de classifier et renseigner leurs données. GO n'est pas totalement considéré comme une ontologie selon la définition informatique stricte du terme car il ne possède pas de règles d'inférence complexes, ne se basant pas sur une description logique de ses concepts et sur des inférences simples de type \textit{est-un} ou \textit{est-une-partie-de}. Il est davantage mis en avant comme un vocabulaire standardisé des concepts mis en jeu dans les recherches afin de mettre en place un espace commun de termes précis et définis, possédant une hiérarchie établie. Il permet donc de mettre en relation des bases de données hétérogènes respectant ses codes ontologiques et donc d'effectuer des opérations de requêtes croisées ou de comparaison. Progressivement de nombreuses autres "bio-ontologies" sont apparues dans la lignée de GO et plusieurs d'entre-elles ont permis de mettre en avant, via le simple mécanisme d'inférence et de mise en relations des données hétérogènes, de nouvelles avancées en biologie \cite{yoshikawa_drug_2004, stenzhorn_biotop_2008, smith_obo_2007}.

Enfin, plusieurs programmes de visualisation analytique se sont appuyés sur la mise en place d'une sémantique décrivant les concepts mis en jeu \cite{rysavy_dive:_2014}. Cette représentation participe à la formalisation des concepts analysés et instaure un premier lien entre les différents modules d'un programme cherchant à partager visualisation et analyses au sein d'un même espace de travail. DIVE constitue un premier exemple de programme possédant un procédé logiciel incluant la mise en place automatique d'une ontologie afin de classer les données et de refléter le modèle de données utilisé par les librairies utilisées durant la session de visualisation et d'analyse. Le parseur mis en place dans DIVE traduit une architecture .NET \url{https://microsoft.com/net} en une structure de donnée hiérarchique. Chaque valeur de donnée est partagée entre l'architecture .NET et les représentations correspondantes dans DIVE permettant une modification/évolution dynamique de celle-ci pendant l’exécution du programme. Cette solution permet également de mettre en place des méthodes de requêtes sur les valeurs ou les relations entre les objets représentés et donc de mettre en avant de nouvelles informations sur les sets de données étudiés. DIVE se rapproche par plusieurs aspects de nos travaux, cependant, plusieurs points distinguent nos deux approches. Tout d'abord, afin de permettre un pré-traitement et une harmonisation des données manipulées, notre ontologie est pré-définie, imposant un contexte de travail précis et fixe. Cela favorise la mise en place de nouveaux modules car l'utilisateur sait quels concepts et propriétés sont mis en jeu dans notre processus. Comme nous l'avons fait remarquer, notre ontologie garde une certaine souplesse puisqu'il est aisément possible de l'étendre à de nouveaux concepts et propriétés suivant les besoin. Il est cependant nécessaire de garder le vocabulaire de base de l'ontologie pour pouvoir profiter de toutes les fonctions en découlant. Notre ontologie est spécifique à notre domaine d'étude, qui est la biologie structurale et plus particulièrement la visualisation et l'analyse de données de modèles de molécules dynamiques ou non. DIVE est capable de fournir une architecture plus globale puisque s'adaptant à chaque set de données mis en jeu. L'inférence propre aux ontologies est également limitée dans DIVE puisqu'elle se limite à une notion d'héritage propre au modèle OO de certains langages de programmation.

Les graphes conceptuels sont un premier moyen de formaliser les connaissances \cite{chein2008graph}. Dans cette représentation, les concepts et les relations sont des noeuds reliés par des arcs orientés.

\section{Visualisation analytique de données de biologie structurale}
\label{Sec:visuAnalyticsStructBio}

\subsection{Outils et techniques}

Plusieurs outils ou techniques découlent de ce couplage étroit entre visualisation de données brutes et analyses associées caractéristique de la visualisation analytique \cite{cockburn2008review}:

\begin{enumerate}
    \item "Overview+Detail": Cette technique fournit plusieurs vues, dont une présente une vue globale, dans des espaces distincts. Les vues peuvent être synchrones ou asynchrones et cette asynchronisme peut être propre à la vue globale ou à la vue en détails. Une interaction dans l'une ou les autres des vues n'a donc pas forcément une conséquence dans les autres vues, cependant, dans la plupart des cas, il existe une synchronisation assurant une cohérence et un lien entre les données affichées. Un exemple connu est Google Maps illustré dans la figure X qui présente deux échelles spatiales différentes dans deux fenêtres distinctes. Seule l'interaction dans la fenêtre de contexte globale (la plus grande / principale) a un effet dans la seconde fenêtre.
    \item "Zooming": Cette technique se base cette fois sur une séparation temporelle des vues. Un zoom avant amènera une vue plus détaillée d'une scène alors qu'un zoom arrière apportera une vue plus globale. La transition entre les différentes échelles de vue peut être continue ou discrète et présenter une animation ou non. L'un des problèmes majeur de cette technique est la difficulté pour l'utilisateur d'inverser une action de zoom. En effet, la notion de défaire ou d'annuler une action en informatique fait souvent référence à des actions ayant eu pour effet de modifer l'état des données et non la vue de l'utilisateur.
    \item "Focus+Context": Cette technique permet à l'utilisateur de se concentrer sur une partie intéressante des données visualiser sans perdre le contexte global dans lequel s'inscrivent ces données. Les informations présentées dans le contexte global ne sont pas nécessairement identiques à celles présentées en détail mais les deux échelles d'information peuvent être associées à travers un affichage dynamique simple. Par exemple, si différents sets de données ont leurs entrées liées par au minimum une propriété équivalente, la sélection d'un point dans un set particulier sélectionne également toutes les points correspondants à ce point dans les autres sets. La figure X nous montre par exemple la sélection d'un modèle Y dans un set de données de simulation moléculaire. Toutes les représentations du modèle Y dans l'espace de visualisation ou d'analyse sont également mises en avant. Cet exemple spécifique est appelé \textit{brush-and-link}.
    \item "Cue-based techniques": Cette technique vise à mettre en avant un sous-ensemble de données intéressantes à l'utilisateur en intervenant sur la façon dont ces données sont affichées. A la différence des autres schémas décrits précédemment, elle n'intervient pas sur la taille des données mais peut être utilisé en conjonction de chacune des techniques précédentes. Elle regroupe des méthodes de brouillage visuel des données, d'ajout de labels ou de mise en place d'élément décoratifs divers pour attirer l'attention de l'utilisateur.
\end{enumerate}

La visualisation analytique possède donc les outils nécessaires au couplage visualisation et analyses propres à l'étude d'une simulation moléculaire. En reprenant le schéma de la visualisation analytique proposé par Keim \textit{et al.} il est facile de considérer, dans notre cas, les données en entrée comme les coordonnées 3D des modèles générés à chaque pas de temps de la simulation ainsi que les propriétés physico-chimiques associées. Il est également possible d'y ajouter des résultats d'analyses. En utilisant des rendus visuels définis pour chaque information à observer (rendu 3D navigable pour les coordonnées 3D des complexes moléculaires, plots et graphes pour les données d'analyses, etc...) et en liant ces rendus entre-eux, il est possible de mettre en place des techniques de Focus+Context ou de Overview+Detail détaillés précédemment. Chaque point d'un graphique devra avoir son équivalent représenté dans le rendu visuel 3D afin de pouvoir créer un lien entre les deux et ainsi ressortir un comportement commun lors de la sélection de ce point particulier par l'utilisateur.

\subsection{Application en RV}

Parmi les différentes techniques de visualisation analytique citées précédemment, celle du Focus+Context, et plus spécifiquement de la technique du \textit{brush-and-link}, est un candidat pertinent pour mettre en avant des informations supplémentaires obtenues à partir des analyses indépendantes effectuées sur une simulation. En biologie structurale, les résultats d'analyse peuvent souvent être représentés au travers de nuages de points, d'histogrammes ou de diagrammes à bandes. Ces représentations s'adaptent particulièrement bien à la technique du \textit{brush-and-link} puisque la sélection de sous-ensembles y est aisée. De plus, la perspective d'une fenêtre dynamique regroupant le contexte global et les détails dans un même ensemble est particulièrement adapté à un environnement immersif. En effet, l'immersion peut être considérablement impactée, et ce de façon négative, lors du partage spatial du dispositif immersif en plusieurs contextes de travail.
Du point de vue de la visualisation, le but est d'être capable de sélectionner un sous-ensemble structural d'un complexe moléculaire afin de mettre en avant les informations correspondant à ce sous-ensemble dans l'espace d'analyses. Par exemple, et comme illustré dans la figure X, la sélection d'un ou plusieurs modèles de la trajectoire aura pour conséquence de mettre en avant cette sélection au sein de la structure et de mettre en avant le sous-ensemble, si existant, dans les graphiques d'analyse. Chaque point des graphiques est relié à un ensemble structural précis (modèle/chaîne/résidu/atome) et une action sur un ensemble de points aura pour effet d’entraîner une action sur l'ensemble structural correspondant. 
% Cela ne peut se faire que si des liens existent entre les données structurales 3D et les données analytiques. Il doit exister une correspondance étroite et bilatérales entre les éléments présentés dans le rendu visuel de la structure 3D (atomes, résidus, chaînes, protéines,...) et dans le rendu visuel des analyses (points, barres, surfaces,...) afin de permettre une synchronisation des actions entre les deux espaces de rendus.
Ces outils ne sont pas suffisant à eux seuls pour répondre à notre problématique. En effet, l'idée principale de notre approche vise à mettre en place une plateforme logicielle souple et générique afin de prendre en compte un nombre important de données différentes en entrée sans nécessité de nouveaux développements pour assurer son fonctionnement. Cette généricité doit passer par une certaine automatisation des étapes de liaison entre les données hétéroclites que peuvent être des données dédiées à la visualisation 3D et des données provenant d'analyses ou de propriétés physico-chimiques.


\section{Sémantique et données liées}

Parmi les solutions possibles, une première pourrait être de mettre en place un suivi strict des données en assignant des identifiants uniques aux données représentant les mêmes concepts afin de garder une cohérence complète entre chaque élément affiché dans l'espace de visualisation et chaque élément de l'espace d'analyse. L'un des problèmes de cette solution se trouve dans la possibilité pour les espaces d'évoluer rapidement et demande donc ainsi une évolution constante du pool d'identifiants. De plus, aucune information quant à la structure du complexe moléculaire ne serait extractible de ces identifiants, une gestion spécifique cherchant à maintenir la cohérence entre les identifiants du premier espace et les identifiants du second devrait ainsi être mise en place en parallèle.
Une seconde solution est de donner au programme un couche d'abstraction supplémentaire via une définition ontologique des concepts manipulés afin de lui permettre de savoir quel concept est concerné par une éventuelle action (sélection, mise en avant, mise en arrière-plan, ...). Chaque élément avec lequel interagit l'utilisateur correspond à un individu unique dont la nature ou l'une de ses propriétés est mise en avant, soit à travers une représentation visuelle, soit à travers une représentation analytique. Chaque interaction avec cet individu déclenchera un résultat dans tous les espaces affichant au moins une de ses propriétés.
Il est donc nécessaire de mettre en place un vocabulaire générique que manipulera la plateforme et que l'utilisateur aura simplement à remplir avec ses données spécifiques afin de former une base de données permanente décrivant son système. 
Les concepts mis en jeu au sein de la plateforme doivent être correctement définis pour que l'utilisateur sache comment enrichir la base de données qui sera prise en entrée par les modules et afin que les données puissent être liées entre elles de façon optimale. La mise en place d'une ontologie permet que ces deux prérequis soient respectés. Le web sémantique tend à lier et structurer les données de façon à faciliter l'accès aux connaissances qu'elles contiennent. C'est également le but de notre étude et il semble donc adapté de reprendre les principes de cette approche afin de mettre en place la structure de notre base de données. Plusieurs avantages qu'offre la mise en place de données liées et d'une ontologie définissant les concepts étudiées peuvent être énumérés:

\begin{itemize}
    \item La plateforme peut identifier à chaque étape les concepts mis en jeu par l'utilisateur et donc proposer des actions adaptées
    \item L'utilisateur dispose d'une définition précise de l'implémentation des concepts et peut ainsi fournir ses données en accord avec celle-ci
    \item Il est possible de partager les données utilisées puisqu'elles sont clairement définies par une ontologie
    \item Les liaisons entre les concepts définies dans l'ontologie peuvent être utilisés à l'identique pour relier des modules différents (visualisation/analyses/interactions/...)
    \item La base de données primaire créées à partir des données de l'utilisateur peut évoluer à chaque instant et grandir grâce à de nouvelles données obtenues à partir des premières
\end{itemize}

Le langage de base du web sémantique est le \textit{Resource Description Framework} (RDF) qui est un modèle de graphe destiné à décrire de façon formelle des ressources et leurs métadonnées associées. C'est un équivalent des graphes conceptuels (GC) qui possèdent le même but mais disposent d'opérations différentes. Le langage RDF est souvent associé au web sémantique qui l'utilise afin de mettre en place un réseau de données partagées et libres. L'avantage du langage RDF est sa possibilité d'être étendu et structuré grâce à une couche ontologique appelée \textit{Web Ontology Laguage} (OWL) qui reprend les critères standards des ontologies existantes et qui est le support de nombreuses ontologies recensées dans le portails officiels de bio-ontologies largement utilisées dans la communauté scientifique \cite{smith_obo_2007}.

\subsection{Ontologie OWL}

Le langage OWL est une recommandation du W3C venu compléter le langage RDF depuis 2012. Il met en oeuvre certaines logiques de description cherchant à préserver la cohérence des données mises en jeu, d'en déduire des nouvelles connaissance ou d'extraire certaines information d'une base de données RDF que l'ontologie définit. Il est donc possible à travers l'utilisation de OWL de mettre en place des règles de définition et de liaisons des concepts afin que tout nouvel individu de la base de données qui sera créé par la suite respecte un modèle de définition commun. 
Comme le montre la figure X, nous avons essayé de définir de façon complète l'ensemble des concepts que l'utilisateur aurait à manipuler lors de ses activités de visualisation et d'analyses synchronisées. 
Nous l'avons vu auparavant, plusieurs bio-ontologies ont été mises en place ces dernières années. Afin d'avoir une description la plus précise qui soit des concepts biologiques mis en jeu, nous nous sommes appuyés sur une ontologie déjà existante et disponible en ligne décrivant de façon complète les acides aminés et leurs propriétés \url{http://bioportal.bioontology.org/ontologies/AMINO-ACID}. Cette ontologie nous a permis de poser les bases biologiques d'un des principaux concepts de biologie structurale. En rassemblant des informations telles que la taille, l'hydrophobicité ou bien la charge de chaque acide-aminé il est ainsi possible d'extraire rapidement des groupes d'acides aminés possédant les mêmes propriétés et ainsi utiliser ces propriétés pour des raisonnements complexes lors de l'interrogation de la base de données. Parmi les nombreuses autres bio-ontologies disponibles, aucune d'entre-elles parvenait à définir aussi simplement que nous le désirions les autres concepts dont nous avions besoin. Leur complexité souvent beaucoup trop importante aurait rapidement surchargé notre ontologie. Nous avons donc créer notre propre définition de ces concepts et ainsi éviter un surplus considérable de concepts/propriétés inutilisés provenant d'ontologies trop précises. Il est cependant important de noter qu'une extension de notre ontologie est possible et même encouragée. Cette extension pourrait par exemple compléter et étoffer certaines propriétés biologiques intéressantes pour des études précises.
En plus des concepts biologiques cités précédemment, nous avons également cherché à définir tous les concepts d'analyses et d'action qui seront en jeu lors de la session d'utilisation de notre plateforme. Les actions rassemblent toutes les actions que l'utilisateur pourrait vouloir effectuer avec les données qu'il manipule, de façon directe ou indirecte. Elles reprennent essentiellement les commandes permises par les logiciels de visualisation moléculaire et les outils d'analyse que nous utilisons. Dans les concepts analytiques sont rassemblés les éléments graphiques ou abstraits qui rentrent en jeu dans la création et la visualisation d'un résultat d'analyse. Nous ne définissons pas, volontairement, les processus d'analyse eux-mêmes car ceux-ci peuvent être de natures très différentes et possèdent un champ de définition bien trop large et complexe pour le sujet de cette thèse. De plus, nous séparons volontairement la visualisation des résultats d'analyses des processus/modules d'analyses. Cela ne signifie pas que certains processus d'analyses ne seront pas utiliser au sein de la plateforme logicielle, il n'est simplement pas pertinent de les ajouter à l'ontologie dans le but d'uniformiser les rendus 3D et d'analyses.
De nombreuses informations peuvent être extraites de la simple description d'un concept via les jeux d'inférence et de règles propres au langage OWL. Ce dernier décrit plusieurs relations entre les concepts et/ou les propriétés permettant d'obtenir plusieurs descriptions à partir d'une seule. Si la transitivité permettant de dire que {{Pour tout a.... FORMULE MATH}} est l'une des plus utilisée et des plus implémentée dans les langages ontologiques, il est cependant plus rare de trouver les notions de symétrie, de disjonction ou d'inversion. SHOULD WE EXPLAIN ALL OF THEM ?
Une fois l'ontologie mise en place, il est possible d'alimenter la base de connaissance en ajoutant les informations biologiques regroupées par l'expert scientifique. Ces informations vont devoir respecter le vocabulaire et la classification définie par les règles présentes dans l'ontologie OWL.

\subsection{Base de données RDF/(S)}

La description d'un environnement d'intérêt passe par l'analyse de toutes les informations biologiques pouvant être labellisées par une chaîne de caractère ou une valeur et qui correspond à un concept ou une propriété identifiée dans l'ontologie OWL. Chaque information alimentera une base de données RDF organisant toutes les informations, de la même manière que pour les descriptions ontologiques, sous forme de triplets de type Sujet/Propriété/Objet.
Dans le cas qui nous intéresse pour ce travail de thèse, les simulations numériques moléculaires peuvent être découpées en une succession de modèles statiques 3D successifs. Chaque modèle correspond à l'échelle "Model" décrite dans notre ontologie. C'est le groupe structurel le plus large que nous ayons défini dans les composants structurels biologiques. Plusieurs triplets présents dans notre base de données finale sont illustrés dans la figure X. Chaque atome possède une position 3d ainsi qu'un résidu auquel il appartient et indirectement, de la même manière, une chaîne et un modèle de référence grâce aux règles d'inférence. Cette information n'a donc pas besoin d'être spécifiée puisque les règles d'inférence instaure, entre autres, le fait qu'un résidu est l'un des composants d'une chaîne qui est elle-même composante d'un modèle. Cette information n'est donc pas présente dans la base de données sous forme d'un triplet explicite de type \textit{ATOM\_1234 my\_ontology:belongs\_to CHAIN\_12} mais ce triplet pourra être obtenu lors d'une requête cherchant à identifier par exemple l'identifiant de la chaîne dont l'atome est l'un des composants. Toutes les propriétés géométriques (position, angles, distance, etc...), physico-chimiques (accessibilité, charge partielle, liaisons, etc...) ou analytiques (énergie, RMSD, température, etc...) pouvant être exportées des jeux de données étudiés sont donc intégrés dans a base de données et surtout associés aux individus créés à partir de la simulation. Ces individus sont des instances des concepts définis dans l'ontologie. Ils forment la population de la base de données et la majorité des propriétés définies se rapportent à eux.
Au-delà du stockage et de la mise à disposition des données suivant les règles pré-définies, la base de donnée RDF et plus particulièrement l'ontologie OWL la décrivant permettent de mettre en place des moteurs d'interprétation de commandes ou requêtes qui vont avoir pour objectif de récupérer une liste d'individus respectant un certain nombre de propriétés énoncées.

\subsection{Requêtes SPARQL comme base logicielle}

Le langage de requête de RDF/OWL est le SPARQL. C'est une autre recommandation de l'organisme W3C qui a été créé de façon à interroger de façon optimale les bases de données RDF de la même façon que ce que peut faire le SQL avec les bases de données du même nom. SPARQL est aussi un langage permettant de modifier une base de données en ajoutant, éditant ou supprimant des informations. A la différence du SQL, SPARQL se base essentiellement sur le format en triplets des bases de données RDF et la majorité de ses requêtes repose sur la mise en place d'un schéma de correspondance entre triplets sujet/prédicat/objet. Il n'y a pas de contraintes de typage pour la colonne objet qui est habituellement implicite ou définie par l'ontologie. Dans le même esprit, l'ontologie est directement intégrée dans les résultats de requêtes et le schéma de données n'a donc pas besoin d'être appliqué de façon séparée. SPARQL fournit également plusieurs opérations sur les résultats comme SORT, JOIN, DISTINCT, qui permettent un traitement direct des résultats afin de les classer ou filtrer suivant les besoins... (voir https://www.cambridgesemantics.com/semantic-university/sparql-vs-sql-intro) De nombreuses librairies peuvent servir d'interfaçage entre le langage de programmation utilisé et une base de données RDF ainsi que son point d'accès SPARQL correspondant permettant d'exposer des requêtes.
Notre base de données est hébergée dans un serveur Virtuoso accessible depuis le réseau afin de garantir un accès privilégié et optimisé à nos données. Ce serveur possède l'ensemble des informations obtenues à partir de la trajectoire d'une simulation moléculaire test reprenant l'évolution structurelle et énergétique d'une protéine transmembranaire, GLIC, pendant 2ns. Le système contient également un ligand identifié de GLIC, le bromoforme, dont on cherche à connaitre le mode de liaison et surtout son impact sur la forme de GLIC au cours du temps. Le solvant utilisé pour la simulation est un modèle simplifié d'eau.

\subsection{Moteur de conversion mots-clés -> RDF -> commandes}

Les conditions immersives dans lesquelles nous plaçons ce travail nous obligent à mettre en place des techniques d'interaction adaptées aux environnements utilisés. Comme évoqué dans l'introduction, les interactions en réalité virtuelle offrent de nouvelles possibilités aux utilisateurs qui peuvent interagir de façon intuitive et directe avec leurs données. Une des techniques d'interaction plébiscitée au sein des environnements immersifs est la commande vocale. Elle traduit une phrase ou un groupe de mots édicté par l'utilisateur en une commande interprétable par la suite logicielle déclenchant ainsi une action appropriée. Les commandes vocales ont également pour intérêt de pouvoir être combinées à des commandes gestuelles qui auront pour effet d'apporter un filtre supplémentaire sur le champ de sélection des objets virtuels concernés par exemple.
Les actions identifiées au sein de notre programme impliquent pour une majorité d'entre elles une activité appliquée à un groupe structurel précis de l'ensemble moléculaire observé. Or ces groupes structurels peuvent être identifiés par des identifiants ayant un sens biologique (les acides aminés sont, de façon conventionnelle, numérotés séquentiellement au sein d'une chaîne de la partie N-terminale de la protéine vers la partie C-terminale), des identifiants uniques au sein de la base de données RDF ou bien leurs propriétés. Ainsi, pour interpréter les commandes émises en langage naturel par l'utilisateur, utilisant un vocabulaire spécifique du domaine avec un haut niveau d'abstraction, nous avons besoin d'une représentation qui peut porter la complexité des connaissances du domaine et relier les objets désignés par l'utilisateur aux objets virtuels d'intérêts.
Grâce à l'ontologie que nous avons mise en place, il est possible d'utiliser les capacités de raisonnement d'OWL ainsi que la puissance de SPARQL afin d'élaborer un moteur de conversion qui traduirait une commande vocale de l'utilisateur vers une commande spécifique appliquée de façon synchrone dans l'espace de visualisation et l'espace d'analyses.
Ce moteur peut être décomposé en 3 parties :

\subsubsection{Reconnaissance vocale via Sphinx}

La reconnaissance vocale en elle-même se fait via Sphinx, un logiciel de reconnaissance vocale basée sur des dictionnaires établies auparavant qui listent l'ensemble des termes qui peuvent être identifiés lors d'une session. Ce dictionnaire se doit d'être le plus complet possible afin de prendre en compte l'ensemble du vocabulaire spécialisé que pourrait utiliser l'utilisateur. Sphinx possède un module VRPN propre qui nous permet de l'intégrer aisément dans notre programme. Sphinx analyse donc une commande vocale utilisateur et la fait correspondre au dictionnaire chargé afin d'en sortir des mots ou groupe de mots spécifiques. Ces mots sont ensuite envoyé au moteur de conversion que nous avons développé.

\subsubsection{Classification des mots-clés}

Une fois la réception des mots-clés effectuée, notre moteur va catégoriser chaque mot reçu. Cette catégorisation se base sur le découpage de notre base de données qui différencie cinq catégories de mots pouvant être retrouvés dans une commande vocale:
\begin{itemize}
	\item Action
	\item Composant
	\item Identifiant
	\item Propriété
	\item Représentation
\end{itemize}

Cette classification se fait via des requêtes SPARQL spécifiques pour chaque catégorie. Alors que les catégories action, composant, propriété et représentation peuvent être identifiée seule, la catégorie identifiant est directement liée à un composant. Étant donné la possible redondance des identifiants dans une base de donnée de simulation moléculaire puisque l'intégralité d'un système y est retrouvé autant de fois qu'il y a d'unité de temps découpant la trajectoire, il est nécessaire d'avoir une association forte entre un identifiant et le composant, quel que soit son niveau structurel. Un identifiant ne pourra donc être classé comme tel que si un composant existe, et seulement si ce composant possède effectivement l'identifiant demandé. Les commandes SPARQL permettant de dire si un mot-clé appartient ou non à une catégorie utilise l'opérateur "ASK" qui prend en argument un ou plusieurs triplets et retourne un booléen "true" si l'ensemble des triplets se vérifie (existe) dans la base de données et "false" si au moins un triplet n'est pas présent. Les cinq requêtes SPARQL formulées pour la classification sont donc construites sur la même forme illustrée dans la figure/équation X. Il est important de préciser, que de la même manière que d'autres requêtes SPARQL standards, les règles de raisonnement et d'inférence sont utilisées lors des requêtes. La requête \textit{Alanine rdfs:subClassOf my\_ontology:Biological\_component} renverra donc "true" malgré l'absence de lien direct, le concept "Residue" se situant entre ces deux concepts.
SHOULD WE DESCRIBE EACH CATEGORY ???

\paragraph{Action}

C'est le concept le plus simple à identifier parmi les mots-clés car il ne nécessite aucune association avec d'autres mots-clés dans notre représentation ontologique. Une liste précise d'action a été identifiée et une correspondance simple nous permet de savoir si le mot employé est une action ou non.

\paragraph{Composant}

Un composant est un ensemble d'atomes ou un unique atome identifié dans notre ontologie, soit par son niveau hiérarchique (modèle, chaîne, résidu, atome), soit par sa désignation directe (carbone, alanine, eau). Lorsqu'un composant est identifié, on cherchera toujours l'éventuelle présence d'un ou de plusieurs identifiants associés au sein de la liste de mots-clés extraite de la commande vocale.

\paragraph{Identifiant}

Un identifiant ne peut être trouvé seul, il est toujours associé à un concept de type "Composant" qu'il désigne. Les identifiants ont aussi la particularité de ne pas être recherché à un niveau ontologique mais au niveau de la base de données elle-même puisqu'il n'existe pas de listes d'identifiants fixes dans notre ontologie, ces derniers variant significativement entre les modèles PDB que nous pourrions traiter. Lorsqu'un couple composant/identifiant existe au sein de la base de données, ces deux éléments sont regroupés afin de constituer un filtre précis de sous-ensemble structurel.

\paragraph{Propriété}

Une propriété met en avant la particularité chimique, physique, biologique ou géométrique d'un composant. Les propriétés, à la différence des identifiants, constituent une liste finie dans notre ontologie et n'ont donc pas besoin d'être associé à un composant afin d'être identifiées. Cependant, certaines propriétés sont directement associées à un niveau hiérarchique précis au sein d'une protéine. On parlera par exemple d'un résidu hydrophobe mais jamais d'une chaîne hydrophile. La cohérence d'une propriété avec les concepts "Composant" identifiés dans la commande doit donc être vérifiée. 
Les propriétés vont agir comme un filtre direct sur le sous-ensemble structurel sur lequel l'utilisateur veut agir. Ce filtre agira soit en unique sélecteur, rassemblant tous les individus appartenant à un groupe hiérarchique possédant la propriété identifiée, soit en filtre supplémentaire d'un sous-ensemble structurel ayant été identifié en parallèle grâce à d'autres concepts "Composant".

\paragraph{Représentation}

Une représentation est une propriété visuelle associée directement et presque exclusivement à l'espace de visualisation. Les mots-clés désignant des concepts "Représentation" sont associés aux actions centrées sur les changements de visualisation. Ils décrivent les états visuels sur lesquels l'utilisateur voudrait intervenir. Ils rassemblent par exemple les méthodes de représentation utilisés dans le visualiseur moléculaire ainsi que les couleurs utilisées.

Une fois que chaque mot-clé est identifié, il est nécessaire d'identifier les différents niveaux de la commande de l'utilisateur.

\subsubsection{Composition de la commande}

Au sein de notre programme, une commande énoncée par l'utilisateur comporte comme base obligatoire une action qui décidera directement de l'événement qui sera déclenchée dans l'espace de travail. Le concept "Action" comporte dans notre ontologie un certain nombre de concepts associés, sous forme de pré-requis, nécessaires à son exécution. Ces pré-requis sont indispensables car ils se comportent comme des paramètres de la commande exécutée. Une action de type "Color" nécessite par exemple la présence d'une propriété de type "Colors" et d'une suite de mots-clés désignant un sous-ensemble structurel. Ce sous-ensemble structurel peut être obtenu de différentes manières, dépendant directement du critère choisi par l'utilisateur pour filtrer les données sur lesquelles il souhaite appliquer son action. Une première manière de désigner un sous-ensemble structurel est la combinaison d'un composant moléculaire et d'un ensemble d'identifiant (unique, listé ou sous forme de plages continues). Sans identifiant, l'ensemble des individus appartenant au concept indiqué seront pris en compte. Une seconde manière de désigner un sous-ensemble structurel est la possibilité de combiner un composant et une propriété. Cette propriété, dont la nature n'est pas limitée, devra seulement être cohérente avec le concept édicté.
Par défaut, le sous-ensemble structurel désigné dans la commande vocale dépendra directement du jeu de données affiché dans l'espace de travail afin d'éviter une surcharge de précision à donner par l'utilisateur. Par exemple, si deux modèles sont affichés dans l'espace de travail, la commande "Alanine 147" désignera uniquement les alanines dont l'identifiant est 147 parmi les deux modèles affichés et non pas parmi l'ensemble des modèles présents dans la base de données.
Si tous les pré-requis associés à une action sont présents, alors la commande est ordonnée de façon à respecter la syntaxe de l'environnement de visualisation utilisé. La dernière étape est donc une simple transformation des concepts et des individus en une formulation compréhensible par les espaces de travail.

\includegraphics[width=.75\linewidth]{./figures/visu_ana_semantic_schema.pdf}

\section{Preuve de concept}

Les différents outils présentés ici nous ont permis de mettre en place un espace de travail combinant visualisation et analyses qui repose exclusivement sur une interrogation constante de la base de données crées à partir de nos données de simulation et respectant l'ontologie OWL mise en place pour ce projet.
La conception de notre programme repose sur un schéma complexe permettant de relier efficacement ses différents composants de manière à les faire communiquer de façon optimale. La base de notre programme se trouve dans les données manipulées, toutes rassemblées au sein de la base de données RDF. Dans le schéma illustré dans la figure X, nous l'avons volontairement placé au centre d'une boucle bi-latérale de communication reliant l'espace de visualisation à l'espace d'analyse. 
% \section{Détachement des interactions standards clavier/souris via sémantique}
% \label{Sec:ModCtrSyl}

% \subsection{Subsection1}
% \label{sec:dynPhaseArti}

% \subsection{Subsection2}
% \label{sec:hypoarticulation}
% \lipsum[1-2]

% \subsection{Subsection3}
% \label{Sec:hypoarticulation}
% \lipsum[1-2]

% \section[Modèle de contrôle alternatif utilisant des gestes de sélection]{Modèle de contrôle alternatif utilisant des gestes de sélection pour le déclenchement des phases VC et CV}
% \lipsum[1-2]

\section{Résumé et conclusion}
\label{sec:ConclusionVisuAna}

